{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1063df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import os\n",
    "import geopy\n",
    "import plotly\n",
    "#import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "import time \n",
    "\n",
    "# Constants\n",
    "DATA_URL = \"https://www.phoenixopendata.com/dataset/cc08aace-9ca9-467f-b6c1-f0879ab1a358/resource/0ce3411a-2fc6-4302-a33f-167f68608a20/download/crime-data_crime-data_crimestat.csv\"\n",
    "LOCAL_DATA_PATH = \"/Users/natebender/Desktop/repo/phx_crime_heat/data/crime_data.csv\"\n",
    "CACHE_FILE_PATH = \"/Users/natebender/Desktop/repo/phx_crime_heat/data/geocode_cache.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb7496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the geocode cache if it exists; otherwise, initialize an empty dictionary\n",
    "try:\n",
    "    cache_df = pd.read_csv(CACHE_FILE_PATH, index_col='address_zip')\n",
    "    geocode_cache = cache_df.to_dict(orient='index')\n",
    "except FileNotFoundError:\n",
    "    geocode_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36359c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address_zip</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1300 E ALMERIA RD, Phoenix, AZ</th>\n",
       "      <td>33.466771</td>\n",
       "      <td>-112.054083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100 N 15TH ST, Phoenix, AZ</th>\n",
       "      <td>33.512281</td>\n",
       "      <td>-112.050423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400 E HIGHLAND AVE, Phoenix, AZ</th>\n",
       "      <td>33.505666</td>\n",
       "      <td>-112.051749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900 W WOOD ST, Phoenix, AZ</th>\n",
       "      <td>33.410824</td>\n",
       "      <td>-112.209052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N 43RD AVE &amp; W CACTUS RD, Phoenix, AZ</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        latitude   longitude\n",
       "address_zip                                                 \n",
       "1300 E ALMERIA RD, Phoenix, AZ         33.466771 -112.054083\n",
       "5100 N 15TH ST, Phoenix, AZ            33.512281 -112.050423\n",
       "1400 E HIGHLAND AVE, Phoenix, AZ       33.505666 -112.051749\n",
       "6900 W WOOD ST, Phoenix, AZ            33.410824 -112.209052\n",
       "N 43RD AVE & W CACTUS RD, Phoenix, AZ        NaN         NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2619dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url):\n",
    "    \"\"\"\n",
    "    Download CSV data from the provided URL and return it as a pandas DataFrame.\n",
    "    Adjust the dtype of 'INC NUMBER' to string to avoid mixed types warning.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = StringIO(response.content.decode('utf-8'))\n",
    "        # Specify dtype for 'INC NUMBER' column to ensure it's read as a string\n",
    "        df = pd.read_csv(data, dtype={'INC NUMBER': str})\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download data: HTTP {response.status_code}\")\n",
    "\n",
    "def save_data(df, path):\n",
    "    \"\"\"\n",
    "    Save the DataFrame to the specified local path.\n",
    "    \"\"\"\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def update_data(url, local_path):\n",
    "    \"\"\"\n",
    "    Update the local dataset with new entries from the dataset at the provided URL.\n",
    "    Adjust the dtype of 'INC NUMBER' to string when loading existing data.\n",
    "    \"\"\"\n",
    "    # Download the latest data\n",
    "    new_data = download_data(url)\n",
    "    new_data = clean_addresses(new_data)\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        # Load the existing data, specifying dtype for 'INC NUMBER'\n",
    "        existing_data = pd.read_csv(local_path, dtype={'inc_number': str})\n",
    "        \n",
    "        # Combine the new data with the existing data, avoiding duplicates\n",
    "        updated_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['inc_number', 'occurred_on'])\n",
    "        \n",
    "        # Save the combined dataset back to the local path\n",
    "        save_data(updated_data, local_path)\n",
    "        print(\"Data has been updated.\")\n",
    "    else:\n",
    "        # If the local file does not exist, just save the new data\n",
    "        save_data(new_data, local_path)\n",
    "        print(\"Data saved.\")\n",
    "\n",
    "# Example usage\n",
    "# update_data(DATA_URL, LOCAL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125a7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_addresses(df):\n",
    "    \"\"\"\n",
    "    Clean the '100 BLOCK ADDR' column in the DataFrame.\n",
    "    \"\"\"\n",
    "    # Example cleaning step: Replace \"XX\" with \"00\" in addresses\n",
    "    df['100_block_addr'] = df['100_block_addr'].str.replace('XX', '00')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a446b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_geocode(address, zip_code=None):\n",
    "    \"\"\"\n",
    "    Attempt to geocode an address using an external geocoding service.\n",
    "    Appends zip code information to the address to improve accuracy.\n",
    "    Caches results using a combination of address and zip code as the key.\n",
    "\n",
    "    Parameters:\n",
    "    - address: The street address to geocode.\n",
    "    - zip_code: Optional zip code to include in the geocoding request.\n",
    "    \"\"\"\n",
    "    # Formulate the query with the zip code, if provided\n",
    "    address_query = f\"{address}, Phoenix, AZ\"\n",
    "#     if zip_code:\n",
    "#         zip_code = str(zip_code)\n",
    "#         address_query += f\", {zip_code}\"\n",
    "    \n",
    "    # Use address_query as the cache key to uniquely identify each geocode request\n",
    "    cache_key = address_query\n",
    "    \n",
    "    # Check cache first to avoid redundant geocoding requests\n",
    "    if cache_key in geocode_cache:\n",
    "        return geocode_cache[cache_key]\n",
    "    \n",
    "    try:\n",
    "       # address_query = f\"{address}, {zip_code}\" if zip_code else address\n",
    "        location = geocode(address_query, timeout=10)\n",
    "        if location:\n",
    "            lat_lon = (location.latitude, location.longitude)\n",
    "            geocode_cache[cache_key] = lat_lon\n",
    "            return lat_lon\n",
    "        else:\n",
    "            geocode_cache[cache_key] = (None, None)\n",
    "            return (None, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding address '{address_query}': {e}\")\n",
    "        geocode_cache[cache_key] = (None, None)\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dccac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache_to_file(cache, file_path):\n",
    "    \"\"\"\n",
    "    Save the geocode cache to a CSV file, including handling cache entries\n",
    "    that are keyed with address and zip code combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - cache: The geocode cache dictionary.\n",
    "    - file_path: Path to the CSV file where the cache is saved.\n",
    "    \"\"\"\n",
    "    cache_data = [{\n",
    "        'address_zip': key,  # The combined address and zip code key\n",
    "        'latitude': lat_lon[0] if lat_lon else None,\n",
    "        'longitude': lat_lon[1] if lat_lon else None\n",
    "    } for key, lat_lon in cache.items()]\n",
    "    \n",
    "    cache_df = pd.DataFrame(cache_data)\n",
    "    # Optionally split 'address_zip' into separate 'address' and 'zip' columns here\n",
    "    cache_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb538b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_addr_processing(df, address_column, cache_file_path, batch_size=2000):\n",
    "    \"\"\"\n",
    "    Processes addresses in batches, applying geocoding, providing progress updates,\n",
    "    and saving the cache periodically.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the addresses to be processed.\n",
    "    - address_column: The name of the column in df that contains the addresses.\n",
    "    - cache_file_path: The file path where the geocode cache will be saved.\n",
    "    - batch_size: The number of addresses to process in each batch.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    geocoded_results = df.apply(lambda x: robust_geocode(x[address_column], x['zip']) if pd.notnull(x[address_column]) else (None, None), axis=1)\n",
    "\n",
    "    # Splitting the tuple results into two separate series for latitude and longitude\n",
    "    df['latitude'], df['longitude'] = zip(*geocoded_results)\n",
    "\n",
    "    #print(df)\n",
    "\n",
    "    # Save cache periodically after each batch\n",
    "    save_cache_to_file(geocode_cache, cache_file_path)\n",
    "        \n",
    "    end_time = time.time()  # End timer\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Batch address processing completed in {elapsed_time:.2f} seconds.\")\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cabf8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"nb_phx_test_app\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, error_wait_seconds=10, max_retries=2, swallow_exceptions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb146a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300, East Almeria Road, Palms Trailer Park, Phoenix, Maricopa County, Arizona, 85006, United States\n",
      "(33.466771, -112.054083)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adjust the rate limiter to use a more conservative delay and a longer timeout\n",
    "# Increasing min_delay_seconds to avoid hitting rate limits\n",
    "# Setting a longer timeout for each geocode call\n",
    "\n",
    "#location = geolocator.geocode(\"175 5th Avenue NYC\")\n",
    "location = geolocator.geocode(\"1300 E ALMERIA RD, Phoenix, AZ, 85006\")\n",
    "\n",
    "print(location.address)\n",
    "print((location.latitude, location.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed9f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = download_data(DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d700203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(df, LOCAL_DATA_PATH)\n",
    "# df_clean = clean_addresses(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23dc2937",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natebender/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3433: DtypeWarning: Columns (0,1,2,3,4,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"/Users/natebender/Desktop/crimestat_copy.csv\",dtype={'inc_number': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd58050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527233"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e0ab53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.columns = [col.lower().replace(' ', '_') for col in df_test.columns]\n",
    "df_test['zip'] = df_test['zip'].astype(str)\n",
    "df_test = df_test.dropna(subset=['inc_number'])\n",
    "# Ensure 'occurred_on' is datetime and create a string version for display\n",
    "df_test['occurred_on'] = pd.to_datetime(df_test['occurred_on'])\n",
    "df_test['occurred_on_str'] = df_test['occurred_on'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c78a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.iloc[:100]\n",
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00da9694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inc_number             0\n",
      "occurred_on            0\n",
      "occurred_to           18\n",
      "ucr_crime_category     0\n",
      "100_block_addr         0\n",
      "zip                    0\n",
      "premise_type           0\n",
      "grid                   0\n",
      "occurred_on_str        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test = clean_addresses(df_test)\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c3ec5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(subset=['occurred_on'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e528bac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_addr_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m100_block_addr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCACHE_FILE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [8], line 22\u001b[0m, in \u001b[0;36mbatch_addr_processing\u001b[0;34m(df, address_column, cache_file_path, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgeocoded_results)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#print(df)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save cache periodically after each batch\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43msave_cache_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeocode_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End timer\u001b[39;00m\n\u001b[1;32m     25\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# Calculate elapsed time\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [7], line 10\u001b[0m, in \u001b[0;36msave_cache_to_file\u001b[0;34m(cache, file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_cache_to_file\u001b[39m(cache, file_path):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Save the geocode cache to a CSV file, including handling cache entries\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    that are keyed with address and zip code combinations.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    - file_path: Path to the CSV file where the cache is saved.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     cache_data \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_zip\u001b[39m\u001b[38;5;124m'\u001b[39m: key,  \u001b[38;5;66;03m# The combined address and zip code key\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m: lat_lon[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m lat_lon \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m: lat_lon[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m lat_lon \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     } \u001b[38;5;28;01mfor\u001b[39;00m key, lat_lon \u001b[38;5;129;01min\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m     16\u001b[0m     cache_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cache_data)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Optionally split 'address_zip' into separate 'address' and 'zip' columns here\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [7], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_cache_to_file\u001b[39m(cache, file_path):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Save the geocode cache to a CSV file, including handling cache entries\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    that are keyed with address and zip code combinations.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    - file_path: Path to the CSV file where the cache is saved.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     cache_data \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_zip\u001b[39m\u001b[38;5;124m'\u001b[39m: key,  \u001b[38;5;66;03m# The combined address and zip code key\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mlat_lon\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m lat_lon \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m: lat_lon[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m lat_lon \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     } \u001b[38;5;28;01mfor\u001b[39;00m key, lat_lon \u001b[38;5;129;01min\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m     16\u001b[0m     cache_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cache_data)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Optionally split 'address_zip' into separate 'address' and 'zip' columns here\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "df_test = batch_addr_processing(df_test, '100_block_addr', CACHE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd18bdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test[\"occurred_on\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad9648",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add a trace for each unique date\n",
    "for date in unique_dates:\n",
    "    df_filtered = df_test[df_test['occurred_on_str'] == date]\n",
    "    if not df_filtered.empty:  # Check if filtered DataFrame is not empty\n",
    "        # Create the text for the hover tooltip\n",
    "        hover_text = df_filtered.apply(lambda row: f\"Address: {row['100_block_addr']}<br>Latitude: {row['latitude']}, Longitude: {row['longitude']}<br>Date: {date}<br>Crime Category: {row['ucr_crime_category']}\", axis=1)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scattermapbox(\n",
    "                lat=df_filtered['latitude'],\n",
    "                lon=df_filtered['longitude'],\n",
    "                mode='markers',\n",
    "                marker=go.scattermapbox.Marker(size=9),\n",
    "                name=date,\n",
    "                text=hover_text,  # Use the custom hover text\n",
    "                hoverinfo='text',  # Ensure only the custom text is displayed on hover\n",
    "                visible=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Debug: Ensure at least one trace is set to visible\n",
    "if fig.data:\n",
    "    fig.data[0].visible = True  # Making the first trace visible\n",
    "\n",
    "# Add and configure the slider\n",
    "steps = []\n",
    "for i, date in enumerate(unique_dates):\n",
    "    date_str = str(date)  # Explicitly convert `date` to string to avoid TypeError\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)}, {\"title\": \"Date: \" + date_str}],\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle visibility\n",
    "    steps.append(step)\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=[{\"active\": 0, \"steps\": steps}],\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    mapbox_zoom=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05972df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a745b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
